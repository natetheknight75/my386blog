---
layout: post
title:  "Scraping NFL Weather Data to Investigate the Impact on Injuries"
author: Nathan Nelson
description: NFL injuries are about to be figured out. 
image : /assets/images/nfl-logo-png-image-1.jpg

---
## Introduction 

Football is a sport that's played in various weather conditions, and it's no secret that these conditions can have an impact on the game. But what about the players? In this blog post, we'll walk through the process of gathering NFL weather data, injury records and stadium details in order to investigate the potential impact on palying surfaces for player injuries or if it is another variable like weather. We'll also discuss the ethical considerations and good web scraping practices implemented throughout the process.


## Data Collection

The two main sources of data for this project are the NFLWeather website and the Kaggle dataset containing injury records and field type. The NFLWeather website provides historical weather data for each game, including temperature, forecast, and wind conditions. The Kaggle dataset contains injury records for NFL players, including details such as the injury type, game location. Before proceeding with the data collection, it's important to ensure that we're adhering to ethical web scraping practices and respecting the websites' terms of use.


![Figure](https://raw.githubusercontent.com/natetheknight75/my386blog/main/assets/images/ethic.jpg)

## Ethical Considerations and Good Web Scraping Practices

When scraping data from websites, it's essential to follow ethical guidelines and respect the website's terms and conditions. To do this, we checked the terms of use for NFLWeather and Kaggle, ensuring that we were allowed to collect and use the data for our project. Additionally, I made sure to implement good web scraping practices, such as:
1. Sending requests at a reasonable rate to avoid putting unnecessary stress on the server.
2. Using a session object to minimize the number of connections made to the server.
3. Parsing the website's content responsibly by targeting specific elements and avoiding unnecessary requests.


![Figure](https://raw.githubusercontent.com/natetheknight75/my386blog/main/assets/images/download-_3_.jpg)

## Data Scarpping and Cleaning

After ensuring ethical web scraping practices, we extracted the weather data from NFLWeather for the 2018 and 2019 seasons. This data was then cleaned and preprocessed to remove irrelevant columns, standardize temperature values, and filter out indoor stadium games where weather conditions were not applicable. 

```
---
# Week 1 2018 weather URL
BC_url = "http://www.nflweather.com/en/week/2018/week-1/"

# Send a GET request to the URL and get the HTML content
response = requests.get(BC_url)
html_content = response.content

# Parse the HTML content with BeautifulSoup
soup = bs(html_content, "html.parser")

# Extract the first table from the HTML content
table = soup.find_all("table")[0]

# Convert the table to a pandas dataframe
df = pd.read_html(str(table))[0]

# Select only the columns we want to keep
df = df[["Home", "Away", "Forecast", "Extended Forecast", "Wind"]]

# Drop rows where "DOME" appears in the "Forecast" column
df = df[df["Forecast"] != "DOME"]

# Split the temperature value into its own column
df[["Temperature (F)", "Forecast Description"]] = df["Forecast"].str.split(" ", 1, expand=True)

# Remove the "f" character from the temperature values
df["Temperature (F)"] = df["Temperature (F)"].str.replace("f", "")

# Convert the temperature values to numeric type
df["Temperature (F)"] = pd.to_numeric(df["Temperature (F)"])

# Drop the "Forecast" and "Wind" columns
df = df.drop(["Forecast", "Extended Forecast", "Wind"], axis=1)

# Add a new column "Dataset" with a constant value
df = df.assign(Dataset="2018_week1")

# Print the resulting dataframe
df
---
```

Then once I found this suitable way of extracting the data from a single page I created it into a for loop to extract the weather from the entire regular season first with the 2018 year. 

```
---
#Now that I figured out how I wanted to the data to perform on a single page of the website I wanted to do a for loop to 
#cycle through the different weeks of the NFL season. 



# Define the base URL
base_url = "http://www.nflweather.com/en/week/2018/week-{week}/"

# Create a list of dataframes
dfs = []

# Loop over the weeks and create a dataframe for each week
for week in range(1, 18):
    # Construct the URL for the week
    url = base_url.format(week=week)
    
    # Send a GET request to the URL and get the HTML content
    response = requests.get(url)
    html_content = response.content

    # Parse the HTML content with BeautifulSoup
    soup = bs(html_content, "html.parser")

    # Extract the first table from the HTML content
    table = soup.find_all("table")[0]

    # Convert the table to a pandas dataframe
    df = pd.read_html(str(table))[0]

    # Select only the columns we want to keep
    df = df[["Home", "Away", "Forecast", "Extended Forecast"]]

    # Drop rows where "DOME" appears in the "Forecast" column...don't need indoor stadium weather data
    df = df[df["Forecast"] != "DOME"]

    # Split the temperature value into its own column
    df[["Temperature (F)", "Forecast Description"]] = df["Forecast"].str.split(" ", 1, expand=True)

    # Remove the "f" character from the temperature values
    df["Temperature (F)"] = df["Temperature (F)"].str.replace("f", "")

    # Convert the temperature values to numeric type
    df["Temperature (F)"] = pd.to_numeric(df["Temperature (F)"])

    # Drop the "Forecast" and "Extended Forecast" columns
    df = df.drop(["Forecast", "Extended Forecast"], axis=1)

    # Add a new column "year_week" with a constant value
    df = df.assign(year_week=f"2018_week{week}")
    
    # Append the dataframe to the list
    dfs.append(df)

# Assign the dataframes to variables
df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17 = dfs

---
```


Then I did this same for the 2019 season by just simply changing the URL to "nflweather.com/en/week/2019/week-{week}/" and running the same code.

Now I just needed to combine them all together in to one dataframe. 

```
---
# Combine all dataframes in ndfs
cndf = pd.concat(ndfs)

# Combine all dataframes in dfs
cdf = pd.concat(dfs)


# Concatenate the two dataframes vertically
wnfl = pd.concat([cdf, cndf])

# Reset the index of the combined dataframe
wnfl = wnfl.reset_index(drop=True)
---
```

Then after doing some minor cleaning that I won't go in to here I was able to then combine this weather data to the injury data provided from Kaggle that also pertained the type of field surface played on.

![Figure](https://raw.githubusercontent.com/natetheknight75/my386blog/main/assets/images/conclu.jpg)


## Conclusion

In this blog post, we've discussed the process of gathering NFL weather data and injury records, while ensuring ethical web scraping practices. By combining these datasets, we now have a valuable resource for investigating the potential impact of whether players really are injured more due to turf fields or if it is other lurking variables. Stay tuned for our next blog post, where we'll dive into an exploratory data analysis of this data to uncover interesting insights and trends.


## GitHub Repository

You can find all the code and datasets used in this project on our [GitHub repository.](https://github.com/natetheknight75/nflgt)
